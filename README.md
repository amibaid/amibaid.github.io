###  About me

- **Studying computer science + mathematics** as a [Turing Scholar](https://www.cs.utexas.edu/turing-scholars-honors-program) at UT Austin. I plan to graduate in Spring 2026, and am currently applying to graduate programs!
- **Undergraduate researcher** in the UT Austin Computer Vision Lab, advised by [Prof. Kristen Grauman](https://www.cs.utexas.edu/~grauman/).
-  I am excited about developing intelligent systems that can understand and reason over information from diverse modalities.

---

### Research
My research focuses on audio-visual multimodal learning, with a focus on reliably leveraging audio while mitigating noise and cross-modal confusion.
- **Undergraduate honors thesis (in progress):** developing a preference optimization framework to reduce cross-modal hallucination in audio-visual language models.  
- **Action2Sound (ECCV 2024, Oral) [[link]](https://vision.cs.utexas.edu/projects/action2sound/):** an ambient-aware video to audio generation approach that explicitly disentangles the action sound from the ambient sounds.
- **Self-Supervised Visual-Acoustic Matching (NeurIPS 2023, acknowledged) [[link]](https://vision.cs.utexas.edu/projects/ss_vam/):**  a self-supervised visual acoustic matching method that re-synthesizes audio to match a target sceneâ€™s acoustics. 

---

### Internships

- **Engineering intern, Stripe (summer 2025):** extended Stripe's LLM-based compliance detection system to support image understanding on merchant websites.  
- **Software engineering intern, Salesforce (summer 2024):** automated a key workflow in Salesforce's internal Temporal platform and contributed to the open-source [Terraform Temporal provider](https://github.com/platacard/terraform-provider-temporal).  

---

### Other projects

- **Gaze-centered Egocentric Video Representations [[GitHub]](https://github.com/amibaid/381V-final-project):** built a gaze-aware preprocessing pipeline that reallocates resolution around gaze, improving efficiency in egocentric video QA.

